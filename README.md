# ğŸ“œ OCR & Diacritization Project By Tsuriel Vizel And Avraham Guez

<a name="project"></a>

Welcome to the **OCR & Diacritization** project! ğŸ¯  
This repository contains all the data, scripts, and trained models used to build and run two separate AI models:  
1. **OCR Model** â€“ to recognize text from images with 93 % accuracy.  
2. **Diacritization Model** â€“ to add *nikkud* (vowel marks) to Hebrew text with 93 % accuracy.  

Below youâ€™ll find a full breakdown of the repository structure and the purpose of each folder and file.  

---

## ğŸ“‘ Table of Contents
- [Project Overview](#project)
- [ğŸ“‚ Project Structure](#project-structure)
  - [DiacData](#diacdata)
  - [OcrData](#ocrdata)
  - [Models](#models)
  - [UseModels](#usemodels)
  - [tools](#tools)
- [ğŸ Main Python Scripts](#main-python-scripts)
- [ğŸ”„ Typical Workflow](#typical-workflow)
- [ğŸ’¡ Usage](#usage)
- [ğŸ“Concrete Example](#concrete-example)
- [ğŸ“š References](#references)

---

<a name="project-structure"></a>
## ğŸ“‚ Project Structure

<a name="diacdata"></a>
### 1. **DiacData** ğŸ“š  
Contains all the **text data** used to train the *Diacritization Model*.

- Holds **two versions** of Hebrew text:  
  1. **With nikkud** âœ… (vowel marks).  
  2. **Without nikkud** âŒ (plain consonantal Hebrew).  
- The model learns from these **paired examples** how to insert the correct diacritical marks into plain text.

ğŸ›  **How we built the dataset:**  
We prepared the diacritization dataset from scratch:  
1. **Download source texts** in `.txt` format from **[sefaria.org](https://www.sefaria.org)** â€” a large online library of Hebrew texts.  
2. Keep the **original version with nikkud** as the *target* text for the model.  
3. Run our custom Python script **`EraseDiac.py`** to remove all diacritics (nikkud) from the text, producing the *input* text.  
4. Store both the *with-nikkud* and *without-nikkud* versions in `DiacData`.

ğŸ“Œ **Why this matters:**  
- Hebrew without nikkud can be highly ambiguous â€” the same word can have many possible readings.  
- By training the model on side-by-side examples of text **with and without** diacritics, it learns to restore the correct nikkud automatically.  
- This makes the system very useful for restoring pronunciation guides in educational, liturgical, or linguistic contexts.

âœ¨ The resulting dataset serves as the **foundation** for the `Diacmodel.py` training script.

---

<a name="ocrdata"></a>
### 2. **OcrData** ğŸ–¼ï¸  
Stores the **training and testing data** for the OCR model.  

- Contains **images of Hebrew text** ğŸ“„ and their corresponding label files.  
- These datasets are used by `OcrModel.py` to train the OCR model to recognize **printed or handwritten Hebrew text**.  

ğŸ›  **How we built the dataset:**  
We created our own **custom â€œhome-madeâ€ dataset** from Hebrew books.  
1. We downloaded Hebrew texts in `.txt` format from **[sefaria.org](https://www.sefaria.org.il/texts)**.  
2. Using the Python library **[trdg](https://github.com/Belval/TextRecognitionDataGenerator)** (*Text Recognition Data Generator*), we transformed these raw text files into a large collection of synthetic training images.  
3. This process allowed us to create a dataset of **~10,000 samples** ğŸ“¦.  

ğŸ“Œ **Command used to generate the dataset**:  
```bash
trdg -i hebrew_1.txt -fd fonts/ -c 10000 -l he -w 1 --output_dir output1/
```
Where:

- `-i hebrew_1.txt` â†’ Input Hebrew text file.  
- `-fd fonts/` â†’ Folder containing Hebrew fonts.  
- `-c 10000` â†’ Number of images to generate.  
- `-l he` â†’ Language code for Hebrew.  
- `-w 1` â†’ Words per image (1 in this case).  
- `--output_dir output1/` â†’ Output folder for generated images.  

âœ¨ This gave us full control over the dataset, including the font styles, image size, and number of samples, making our OCR model more robust and accurate.

---

<a name="models"></a>
### 3. **Models** ğŸ†  
This is where the **trained models** are stored after training is complete.  
- Each model is saved in a format ready to be loaded for inference.  
- **Contents:**  
  - `OcrModel` (final OCR model)  
  - `DiacModel` (final diacritization model)  

---

<a name="usemodels"></a>
### 4. **UseModels** ğŸš€  
Contains Python scripts to **use the trained models** for real-world tasks.  
- Scripts to load and run **only the OCR model** ğŸ–‹ï¸  
- Scripts to load and run **only the Diacritization model** âœ¨  
- Scripts to **chain both models** so you can:
  1. Recognize text from an image (OCR).  
  2. Automatically add vowel marks (Diacritization).  

This folder is your **go-to** when you want to *use* the models rather than train them.

---

<a name="tools"></a>
### 5. **tools** ğŸ› ï¸  
A collection of **utility scripts** used throughout the project.  
- Functions for data preprocessing.  
- Helper scripts for training, evaluation, and formatting datasets.  
- Custom Python tools to simplify repetitive tasks.

---

<a name="main-python-scripts"></a>
## ğŸ Main Python Scripts

### **`OcrModel.py`** ğŸ“„  
- Script to train the **OCR model**.  
- Loads `OcrData` dataset, preprocesses it, trains the model, and saves it into `Models`.
- Hyperparameters we used:
 ```bash
EPOCHS = 20
BATCH_SIZE = 32
LEARNING_RATE = 0.0005
IMAGE_HEIGHT = 64 # Resize all images to this height
IMAGE_WIDTH = 256 # Resize all images to this width
NUM_WORKERS = 4 # For DataLoader
TRAIN_VAL_SPLIT = 0.9 # 90% for training, 10% for validation
```

### **`Diacmodel.py`** âœ’ï¸  
- Script to train the **Diacritization model**.  
- Uses `DiacData` (text pairs) to learn how to insert nikkud into plain Hebrew text.  
- Saves the trained model into `Models`.
- Hyperparameters we used:
 ```bash
embed_dim = 128
hidden_dim = 256
num_layers = 2
dropout = 0.3
epochs = 20
batch_size = 16
lr = 0.0001 #We used LROnPlateau nut most of the training was with lr=0.0001
```

---

<a name="typical-workflow"></a>
## ğŸ”„ Typical Workflow

1. **Prepare Data**  
   - Place OCR training images inside `OcrData`.  
   - Place diacritization text data inside `DiacData`.  

2. **Train Models**  
   - Run `OcrModel.py` â†’ generates OCR model.  
   - Run `Diacmodel.py` â†’ generates diacritization model.  

3. **Use Models**  
   - Go to `UseModels` and run the script for the task you need (OCR only, Diacritization only, or both combined).  

---

<a name="usage"></a>
## ğŸ’¡ Usage
```bash
# Run OCR on an image
python UseModels/use_ocr.py  # change path to picture in the script

# Add diacritics to text
python UseModels/use_diac.py "×©×œ×•× ×¢×•×œ×"

# Full pipeline: OCR + Diacritics
python UseModels/usemodels.py 
```

---

<a name="concrete-example"></a>
## ğŸ“Concrete Example
| Step | Input / Output | Description |
|------|---------------|-------------|
| ğŸ–¼ï¸ Input Image | ![Example Hebrew](OcrData/test.jpg) | A Hebrew word image with no diacritics. |
| âœï¸ OCR Output | ×—×›××” | The OCR model recognizes the text without nikkud. |
| ğŸ¯ Final Output | ×—Ö¸×›Ö°×Ö¸×” | The diacritization model adds the correct vowel marks. |

---

ğŸ“Œ The beauty of this pipeline is that **each model can be used independently** or **together** depending on your needs.

---

<a name="references"></a>
## ğŸ“š References
- [Sefaria â€“ A Living Library of Jewish Texts](https://www.sefaria.org)  
- [TRDG â€“ TextRecognitionDataGenerator (GitHub)](https://github.com/Belval/TextRecognitionDataGenerator)  
- [Python Official Documentation](https://docs.python.org/3/)  
- [PyTorch â€” Tensors and Deep Learning (Official)](https://pytorch.org)

---

**Thank you for reading us.**

***Tsuriel and Avraham.***
